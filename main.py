# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G6sBuZsaR64B6xkyy6T4jDsBLoVuOSZx
"""

import pandas as pd
import os
import numpy as np

from milestone1_func import get_game_ids_for_season, save_game_data_to_local, load_data_from_files

"""##2. Feature Engineering I (10%)"""

'''
Using the functionality you created for Milestone 1, acquire all of the raw play-by-play data for the 2015/16 season
all the way to the 2019/20 season (inclusive). Note that the tidied data that you created will be useful for the baseline
models, but you will be creating more features that will require the full raw data in Part 4.

Set aside all of the 2019/20 data as your final test set. You should not touch this until you reach the end of this milestone.
You will use the 2015/16 - 2018/19 regular season data to create your training and validation sets. By the end of this milestone,
you will have built up all the framework you need to process new data, so it should be trivial to process your test data once
you have finished all of the necessary feature engineering. Until Part 7, any reference to the “dataset” will exclusively refer
to the 2015/16 - 2018/19 data.
'''

#directory = "/content/drive/MyDrive/DS-Project/milestone2_data" #change this to your directory


directory = "data"  # change this to your directory: run from root milestone2 dir


'''
1. Acquire all of the raw play-by-play data for the 2015/16 season all the way to the 2019/20 season (inclusive)
2. Set aside all of the 2019/20 data as your final test set.
3. You will use the 2015/16 - 2018/19 regular season data to create your training and validation sets.
'''

# DOESN'T DO ANYTHING IF FILES ALREADY EXIST
seasons = ["20152016", "20162017", "20172018", "20182019", "20192020"]
for season in seasons:
    game_ids = get_game_ids_for_season(season)
    save_game_data_to_local(game_ids, directory)
    print(f"Saved game data for season {season} to {directory}/game_id.json")


# Load saved data into a pandas DataFrame
from feature_engineering_1 import tidy_data


tidied_file_path = os.path.join(directory, 'tidied_training_set.csv')
test_file_path = os.path.join(directory, 'test_set.csv')

if os.path.exists(tidied_file_path) and os.path.exists(test_file_path):
    tidied_training_set = pd.read_csv(tidied_file_path)
    test_set = pd.read_csv(test_file_path)
else:
    data_list = load_data_from_files(directory)
    data = pd.DataFrame(data_list)

    # Add columns for season and game_type from gamePk
    data['season'] = data['gamePk'].apply(lambda x: str(x)[:4])
    data['game_type'] = data['gamePk'].apply(lambda x: str(x)[4:6])

    # TEST SET

    # 2019-2020 season  # TODO: SHOULD WE KEEP ONLY game_type == '02' AS IN training_set?
    test_set = data[data['season'] == '2019'].copy()
    test_set.drop(['season', 'game_type'], axis=1, inplace=True)

    # Save as csv
    test_set.to_csv(test_file_path, index=False)

    # TRAINING AND VALIDATION SET

    # 2015/16 - 2018/19 regular season data
    train_seasons = {'2015', "2016", "2017", "2018"}
    training_set = data[(data['season'].isin(train_seasons)) & (data['game_type'] == '02')].copy()
    training_set.drop(['season', 'game_type'], axis=1, inplace=True)

    tidied_training_set = tidy_data(training_set)
    # To save as csv
    tidied_training_set.to_csv(tidied_file_path, index=False)


tidied_training_set.iloc[1]

# Uncomment to delete json files from directory (can use after saving data as csv)
'''
file_list = os.listdir(directory)

json_files = [f for f in file_list if f.endswith(".json")]

for json_file in json_files:
    file_path = os.path.join(directory, json_file)
    os.remove(file_path)
    print(f"Deleted: {json_file}")
'''


"""#### Feature Engineering 1"""

# SEE feature_engineering_1.py

#checked https://www.nhl.com/gamecenter/nyi-vs-sjs/2015/10/17/2015020071/playbyplay
# and rink side is wrong ...




"""# 4. Feature Engineering II (15% + bonus 5%)"""

